{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d734aef9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T17:18:29.952444Z",
     "iopub.status.busy": "2025-02-02T17:18:29.952010Z",
     "iopub.status.idle": "2025-02-02T17:18:29.984227Z",
     "shell.execute_reply": "2025-02-02T17:18:29.983025Z"
    },
    "papermill": {
     "duration": 0.038567,
     "end_time": "2025-02-02T17:18:29.986287",
     "exception": false,
     "start_time": "2025-02-02T17:18:29.947720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Implementation of 'from lifelines.utils import concordance_index'\n",
    "# because of no internet access(!pip install lifelines) when inferencing with kaggle\n",
    "# src: https://github.com/CamDavidsonPilon/lifelines/blob/47afb1c1a272b0f03e0c8ca00e63df27eb2a0560/lifelines/utils/concordance.py\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class _BTree:\n",
    "\n",
    "    \"\"\"A simple balanced binary order statistic tree to help compute the concordance.\n",
    "\n",
    "    When computing the concordance, we know all the values the tree will ever contain. That\n",
    "    condition simplifies this tree a lot. It means that instead of crazy AVL/red-black shenanigans\n",
    "    we can simply do the following:\n",
    "\n",
    "    - Store the final tree in flattened form in an array (so node i's children are 2i+1, 2i+2)\n",
    "    - Additionally, store the current size of each subtree in another array with the same indices\n",
    "    - To insert a value, just find its index, increment the size of the subtree at that index and\n",
    "      propagate\n",
    "    - To get the rank of an element, you add up a bunch of subtree counts\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, values):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        values: list\n",
    "            List of sorted (ascending), unique values that will be inserted.\n",
    "        \"\"\"\n",
    "        self._tree = self._treeify(values)\n",
    "        self._counts = np.zeros_like(self._tree, dtype=int)\n",
    "\n",
    "    @staticmethod\n",
    "    def _treeify(values):\n",
    "        \"\"\"Convert the np.ndarray `values` into a complete balanced tree.\n",
    "\n",
    "        Assumes `values` is sorted ascending. Returns a list `t` of the same length in which t[i] >\n",
    "        t[2i+1] and t[i] < t[2i+2] for all i.\"\"\"\n",
    "        if len(values) == 1:  # this case causes problems later\n",
    "            return values\n",
    "        tree = np.empty_like(values)\n",
    "        # Tree indices work as follows:\n",
    "        # 0 is the root\n",
    "        # 2n+1 is the left child of n\n",
    "        # 2n+2 is the right child of n\n",
    "        # So we now rearrange `values` into that format...\n",
    "\n",
    "        # The first step is to remove the bottom row of leaves, which might not be exactly full\n",
    "        last_full_row = int(np.log2(len(values) + 1) - 1)\n",
    "        len_ragged_row = len(values) - (2 ** (last_full_row + 1) - 1)\n",
    "        if len_ragged_row > 0:\n",
    "            bottom_row_ix = np.s_[: 2 * len_ragged_row : 2]\n",
    "            tree[-len_ragged_row:] = values[bottom_row_ix]\n",
    "            values = np.delete(values, bottom_row_ix)\n",
    "\n",
    "        # Now `values` is length 2**n - 1, so can be packed efficiently into a tree\n",
    "        # Last row of nodes is indices 0, 2, ..., 2**n - 2\n",
    "        # Second-last row is indices 1, 5, ..., 2**n - 3\n",
    "        # nth-last row is indices (2**n - 1)::(2**(n+1))\n",
    "        values_start = 0\n",
    "        values_space = 2\n",
    "        values_len = 2 ** last_full_row\n",
    "        while values_start < len(values):\n",
    "            tree[values_len - 1 : 2 * values_len - 1] = values[values_start::values_space]\n",
    "            values_start += int(values_space / 2)\n",
    "            values_space *= 2\n",
    "            values_len = int(values_len / 2)\n",
    "        return tree\n",
    "\n",
    "    def insert(self, value):\n",
    "        \"\"\"Insert an occurrence of `value` into the btree.\"\"\"\n",
    "        i = 0\n",
    "        n = len(self._tree)\n",
    "        while i < n:\n",
    "            cur = self._tree[i]\n",
    "            self._counts[i] += 1\n",
    "            if value < cur:\n",
    "                i = 2 * i + 1\n",
    "            elif value > cur:\n",
    "                i = 2 * i + 2\n",
    "            else:\n",
    "                return\n",
    "        raise ValueError(\"Value %s not contained in tree.\" \"Also, the counts are now messed up.\" % value)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._counts[0]\n",
    "\n",
    "    def rank(self, value):\n",
    "        \"\"\"Returns the rank and count of the value in the btree.\"\"\"\n",
    "        i = 0\n",
    "        n = len(self._tree)\n",
    "        rank = 0\n",
    "        count = 0\n",
    "        while i < n:\n",
    "            cur = self._tree[i]\n",
    "            if value < cur:\n",
    "                i = 2 * i + 1\n",
    "                continue\n",
    "            elif value > cur:\n",
    "                rank += self._counts[i]\n",
    "                # subtract off the right tree if exists\n",
    "                nexti = 2 * i + 2\n",
    "                if nexti < n:\n",
    "                    rank -= self._counts[nexti]\n",
    "                    i = nexti\n",
    "                    continue\n",
    "                else:\n",
    "                    return (rank, count)\n",
    "            else:  # value == cur\n",
    "                count = self._counts[i]\n",
    "                lefti = 2 * i + 1\n",
    "                if lefti < n:\n",
    "                    nleft = self._counts[lefti]\n",
    "                    count -= nleft\n",
    "                    rank += nleft\n",
    "                    righti = lefti + 1\n",
    "                    if righti < n:\n",
    "                        count -= self._counts[righti]\n",
    "                return (rank, count)\n",
    "        return (rank, count)\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import numpy as np\n",
    "# from lifelines.utils.btree import _BTree\n",
    "\n",
    "\n",
    "def somers_d(event_times, x, event_observed=None) -> float:\n",
    "    \"\"\"\n",
    "    A measure of rank association between [-1, 1] between a censored variable, event_times,\n",
    "    and another (uncensored) variable, x. -1 is strong anti-correlation, 1 is strong correlation.\n",
    "\n",
    "\n",
    "    event_times: iterable\n",
    "         a length-n iterable of observed survival times.\n",
    "    x: iterable\n",
    "        a length-n iterable to compare against\n",
    "    event_observed: iterable, optional\n",
    "        a length-n iterable censoring flags, 1 if observed, 0 if not. Default None assumes all observed.\n",
    "\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    .. code:: python\n",
    "        from lifelines.datasets import load_rossi\n",
    "        from lifelines.utils\n",
    "\n",
    "        T, E = df['week'], df['arrest']\n",
    "        x = df['age']\n",
    "        somers_d(T, x, E)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    return 2 * concordance_index(event_times, x, event_observed) - 1\n",
    "\n",
    "\n",
    "def concordance_index(event_times, predicted_scores, event_observed=None) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the concordance index (C-index) between a series\n",
    "    of event times and a predicted score. The first is the real survival times from\n",
    "    the observational data, and the other is the predicted score from a model of some kind.\n",
    "\n",
    "    The c-index is the average of how often a model says X is greater than Y when, in the observed\n",
    "    data, X is indeed greater than Y. The c-index also handles how to handle censored values\n",
    "    (obviously, if Y is censored, it's hard to know if X is truly greater than Y).\n",
    "\n",
    "\n",
    "    The concordance index is a value between 0 and 1 where:\n",
    "\n",
    "    - 0.5 is the expected result from random predictions,\n",
    "    - 1.0 is perfect concordance and,\n",
    "    - 0.0 is perfect anti-concordance (multiply predictions with -1 to get 1.0)\n",
    "\n",
    "    The calculation internally done is\n",
    "\n",
    "    >>> (pairs_correct + 0.5 * pairs_tied) / admissable_pairs\n",
    "\n",
    "    where ``pairs_correct`` is the number of pairs s.t. if ``t_x > t_y``, then ``s_x > s_y``, pairs,\n",
    "    ``pairs_tied`` is the number of pairs where ``s_x = s_y``, and ``admissable_pairs`` is all possible pairs. The subtleties\n",
    "    are in how censored observation are handled (ex: not all pairs can be evaluated due to censoring).\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    event_times: iterable\n",
    "         a length-n iterable of observed survival times.\n",
    "    predicted_scores: iterable\n",
    "        a length-n iterable of predicted scores - these could be survival times, or hazards, etc. See https://stats.stackexchange.com/questions/352183/use-median-survival-time-to-calculate-cph-c-statistic/352435#352435\n",
    "    event_observed: iterable, optional\n",
    "        a length-n iterable censoring flags, 1 if observed, 0 if not. Default None assumes all observed.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    c-index: float\n",
    "      a value between 0 and 1.\n",
    "\n",
    "    References\n",
    "    -----------\n",
    "    Harrell FE, Lee KL, Mark DB. Multivariable prognostic models: issues in\n",
    "    developing models, evaluating assumptions and adequacy, and measuring and\n",
    "    reducing errors. Statistics in Medicine 1996;15(4):361-87.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    .. code:: python\n",
    "\n",
    "        from lifelines.utils import concordance_index\n",
    "        cph = CoxPHFitter().fit(df, 'T', 'E')\n",
    "        concordance_index(df['T'], -cph.predict_partial_hazard(df), df['E'])\n",
    "\n",
    "    \"\"\"\n",
    "    event_times, predicted_scores, event_observed = _preprocess_scoring_data(event_times, predicted_scores, event_observed)\n",
    "    num_correct, num_tied, num_pairs = _concordance_summary_statistics(event_times, predicted_scores, event_observed)\n",
    "\n",
    "    return _concordance_ratio(num_correct, num_tied, num_pairs)\n",
    "\n",
    "\n",
    "def _concordance_ratio(num_correct: int, num_tied: int, num_pairs: int) -> float:\n",
    "    if num_pairs == 0:\n",
    "        raise ZeroDivisionError(\"No admissable pairs in the dataset.\")\n",
    "    return (num_correct + num_tied / 2) / num_pairs\n",
    "\n",
    "\n",
    "def _concordance_summary_statistics(event_times, predicted_event_times, event_observed):  # pylint: disable=too-many-locals\n",
    "    \"\"\"Find the concordance index in n * log(n) time.\n",
    "\n",
    "    Assumes the data has been verified by lifelines.utils.concordance_index first.\n",
    "    \"\"\"\n",
    "    # Here's how this works.\n",
    "    #\n",
    "    # It would be pretty easy to do if we had no censored data and no ties. There, the basic idea\n",
    "    # would be to iterate over the cases in order of their true event time (from least to greatest),\n",
    "    # while keeping track of a pool of *predicted* event times for all cases previously seen (= all\n",
    "    # cases that we know should be ranked lower than the case we're looking at currently).\n",
    "    #\n",
    "    # If the pool has O(log n) insert and O(log n) RANK (i.e., \"how many things in the pool have\n",
    "    # value less than x\"), then the following algorithm is n log n:\n",
    "    #\n",
    "    # Sort the times and predictions by time, increasing\n",
    "    # n_pairs, n_correct := 0\n",
    "    # pool := {}\n",
    "    # for each prediction p:\n",
    "    #     n_pairs += len(pool)\n",
    "    #     n_correct += rank(pool, p)\n",
    "    #     add p to pool\n",
    "    #\n",
    "    # There are three complications: tied ground truth values, tied predictions, and censored\n",
    "    # observations.\n",
    "    #\n",
    "    # - To handle tied true event times, we modify the inner loop to work in *batches* of observations\n",
    "    # p_1, ..., p_n whose true event times are tied, and then add them all to the pool\n",
    "    # simultaneously at the end.\n",
    "    #\n",
    "    # - To handle tied predictions, which should each count for 0.5, we switch to\n",
    "    #     n_correct += min_rank(pool, p)\n",
    "    #     n_tied += count(pool, p)\n",
    "    #\n",
    "    # - To handle censored observations, we handle each batch of tied, censored observations just\n",
    "    # after the batch of observations that died at the same time (since those censored observations\n",
    "    # are comparable all the observations that died at the same time or previously). However, we do\n",
    "    # NOT add them to the pool at the end, because they are NOT comparable with any observations\n",
    "    # that leave the study afterward--whether or not those observations get censored.\n",
    "    if np.logical_not(event_observed).all():\n",
    "        return (0, 0, 0)\n",
    "\n",
    "    died_mask = event_observed.astype(bool)\n",
    "    # TODO: is event_times already sorted? That would be nice...\n",
    "    died_truth = event_times[died_mask]\n",
    "    ix = np.argsort(died_truth)\n",
    "    died_truth = died_truth[ix]\n",
    "    died_pred = predicted_event_times[died_mask][ix]\n",
    "\n",
    "    censored_truth = event_times[~died_mask]\n",
    "    ix = np.argsort(censored_truth)\n",
    "    censored_truth = censored_truth[ix]\n",
    "    censored_pred = predicted_event_times[~died_mask][ix]\n",
    "\n",
    "    censored_ix = 0\n",
    "    died_ix = 0\n",
    "    times_to_compare = _BTree(np.unique(died_pred))\n",
    "    num_pairs = np.int64(0)\n",
    "    num_correct = np.int64(0)\n",
    "    num_tied = np.int64(0)\n",
    "\n",
    "    # we iterate through cases sorted by exit time:\n",
    "    # - First, all cases that died at time t0. We add these to the sortedlist of died times.\n",
    "    # - Then, all cases that were censored at time t0. We DON'T add these since they are NOT\n",
    "    #   comparable to subsequent elements.\n",
    "    while True:\n",
    "        has_more_censored = censored_ix < len(censored_truth)\n",
    "        has_more_died = died_ix < len(died_truth)\n",
    "        # Should we look at some censored indices next, or died indices?\n",
    "        if has_more_censored and (not has_more_died or died_truth[died_ix] > censored_truth[censored_ix]):\n",
    "            pairs, correct, tied, next_ix = _handle_pairs(censored_truth, censored_pred, censored_ix, times_to_compare)\n",
    "            censored_ix = next_ix\n",
    "        elif has_more_died and (not has_more_censored or died_truth[died_ix] <= censored_truth[censored_ix]):\n",
    "            pairs, correct, tied, next_ix = _handle_pairs(died_truth, died_pred, died_ix, times_to_compare)\n",
    "            for pred in died_pred[died_ix:next_ix]:\n",
    "                times_to_compare.insert(pred)\n",
    "            died_ix = next_ix\n",
    "        else:\n",
    "            assert not (has_more_died or has_more_censored)\n",
    "            break\n",
    "\n",
    "        num_pairs += pairs\n",
    "        num_correct += correct\n",
    "        num_tied += tied\n",
    "\n",
    "    return (num_correct, num_tied, num_pairs)\n",
    "\n",
    "\n",
    "def _handle_pairs(truth, pred, first_ix, times_to_compare):\n",
    "    \"\"\"\n",
    "    Handle all pairs that exited at the same time as truth[first_ix].\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "      (pairs, correct, tied, next_ix)\n",
    "      new_pairs: The number of new comparisons performed\n",
    "      new_correct: The number of comparisons correctly predicted\n",
    "      next_ix: The next index that needs to be handled\n",
    "    \"\"\"\n",
    "    next_ix = first_ix\n",
    "    while next_ix < len(truth) and truth[next_ix] == truth[first_ix]:\n",
    "        next_ix += 1\n",
    "    pairs = len(times_to_compare) * (next_ix - first_ix)\n",
    "    correct = np.int64(0)\n",
    "    tied = np.int64(0)\n",
    "    for i in range(first_ix, next_ix):\n",
    "        rank, count = times_to_compare.rank(pred[i])\n",
    "        correct += rank\n",
    "        tied += count\n",
    "\n",
    "    return (pairs, correct, tied, next_ix)\n",
    "\n",
    "\n",
    "def _naive_concordance_summary_statistics(event_times, predicted_event_times, event_observed):\n",
    "    \"\"\"\n",
    "    Fallback, simpler method to compute concordance.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def _valid_comparison(time_a, time_b, event_a, event_b):\n",
    "        \"\"\"True if times can be compared.\"\"\"\n",
    "        if time_a == time_b:\n",
    "            # Ties are only informative if exactly one event happened\n",
    "            return event_a != event_b\n",
    "        if event_a and event_b:\n",
    "            return True\n",
    "        if event_a and time_a < time_b:\n",
    "            return True\n",
    "        if event_b and time_b < time_a:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def _concordance_value(time_a, time_b, pred_a, pred_b, event_a, event_b):\n",
    "        if pred_a == pred_b:\n",
    "            # Same as random\n",
    "            return (0, 1)\n",
    "        if pred_a < pred_b:\n",
    "            return (time_a < time_b) or (time_a == time_b and event_a and not event_b), 0\n",
    "        # pred_a > pred_b\n",
    "        return (time_a > time_b) or (time_a == time_b and not event_a and event_b), 0\n",
    "\n",
    "    num_pairs = 0.0\n",
    "    num_correct = 0.0\n",
    "    num_tied = 0.0\n",
    "\n",
    "    for a, time_a in enumerate(event_times):\n",
    "        pred_a = predicted_event_times[a]\n",
    "        event_a = event_observed[a]\n",
    "        # Don't want to double count\n",
    "        for b in range(a + 1, len(event_times)):\n",
    "            time_b = event_times[b]\n",
    "            pred_b = predicted_event_times[b]\n",
    "            event_b = event_observed[b]\n",
    "\n",
    "            if _valid_comparison(time_a, time_b, event_a, event_b):\n",
    "                num_pairs += 1.0\n",
    "                crct, ties = _concordance_value(time_a, time_b, pred_a, pred_b, event_a, event_b)\n",
    "                num_correct += crct\n",
    "                num_tied += ties\n",
    "\n",
    "    return (num_correct, num_tied, num_pairs)\n",
    "\n",
    "\n",
    "def naive_concordance_index(event_times, predicted_event_times, event_observed=None) -> float:\n",
    "    event_times, predicted_event_times, event_observed = _preprocess_scoring_data(\n",
    "        event_times, predicted_event_times, event_observed\n",
    "    )\n",
    "    return _concordance_ratio(*_naive_concordance_summary_statistics(event_times, predicted_event_times, event_observed))\n",
    "\n",
    "\n",
    "def _preprocess_scoring_data(event_times, predicted_scores, event_observed):\n",
    "    event_times = np.asarray(event_times, dtype=float)\n",
    "    predicted_scores = np.asarray(predicted_scores, dtype=float)\n",
    "\n",
    "    # Allow for (n, 1) or (1, n) arrays\n",
    "    if event_times.ndim == 2 and (event_times.shape[0] == 1 or event_times.shape[1] == 1):\n",
    "        # Flatten array\n",
    "        event_times = event_times.ravel()\n",
    "    # Allow for (n, 1) or (1, n) arrays\n",
    "    if predicted_scores.ndim == 2 and (predicted_scores.shape[0] == 1 or predicted_scores.shape[1] == 1):\n",
    "        # Flatten array\n",
    "        predicted_scores = predicted_scores.ravel()\n",
    "\n",
    "    if event_times.shape != predicted_scores.shape:\n",
    "        raise ValueError(\"Event times and predictions must have the same shape\")\n",
    "    if event_times.ndim != 1:\n",
    "        raise ValueError(\"Event times can only be 1-dimensional: (n,)\")\n",
    "\n",
    "    if event_observed is None:\n",
    "        event_observed = np.ones(event_times.shape[0], dtype=float)\n",
    "    else:\n",
    "        event_observed = np.asarray(event_observed, dtype=float).ravel()\n",
    "        if event_observed.shape != event_times.shape:\n",
    "            raise ValueError(\"Observed events must be 1-dimensional of same length as event times\")\n",
    "\n",
    "    # check for NaNs\n",
    "    for a in [event_times, predicted_scores, event_observed]:\n",
    "        if np.isnan(a).any():\n",
    "            raise ValueError(\"NaNs detected in inputs, please correct or drop.\")\n",
    "\n",
    "    return event_times, predicted_scores, event_observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5231361",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T17:18:29.992554Z",
     "iopub.status.busy": "2025-02-02T17:18:29.992068Z",
     "iopub.status.idle": "2025-02-02T17:22:30.524401Z",
     "shell.execute_reply": "2025-02-02T17:22:30.523306Z"
    },
    "papermill": {
     "duration": 240.537606,
     "end_time": "2025-02-02T17:22:30.526545",
     "exception": false,
     "start_time": "2025-02-02T17:18:29.988939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-02 17:18:34,379] A new study created in memory with name: no-name-ed393adb-09a0-4d24-96eb-a3c38aa2bd01\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f70d247e98442a0bff0a9f4e651031a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I 2025-02-02 17:18:49,127] Trial 0 finished with value: 0.6542372019813001 and parameters: {'iterations': 115, 'depth': 9, 'learning_rate': 0.1408039263778209, 'l2_leaf_reg': 2.2347991414391246, 'random_strength': 0.14944250659759895, 'bagging_temperature': 0.5511433935825152, 'border_count': 43}. Best is trial 0 with value: 0.6542372019813001.\n",
      "[I 2025-02-02 17:18:55,548] Trial 1 finished with value: 0.64887670763244 and parameters: {'iterations': 115, 'depth': 6, 'learning_rate': 0.022210486851574358, 'l2_leaf_reg': 3.147913909689499, 'random_strength': 2.050425329029774, 'bagging_temperature': 0.26555598707441086, 'border_count': 234}. Best is trial 0 with value: 0.6542372019813001.\n",
      "[I 2025-02-02 17:19:28,383] Trial 2 finished with value: 0.6622717438107232 and parameters: {'iterations': 719, 'depth': 6, 'learning_rate': 0.01884892696979274, 'l2_leaf_reg': 7.129540530307217, 'random_strength': 1.0704062122598603, 'bagging_temperature': 0.1596179759584443, 'border_count': 91}. Best is trial 2 with value: 0.6622717438107232.\n",
      "[I 2025-02-02 17:19:33,064] Trial 3 finished with value: 0.657211901198621 and parameters: {'iterations': 123, 'depth': 3, 'learning_rate': 0.07652913812418767, 'l2_leaf_reg': 7.525167025259443, 'random_strength': 0.059422150886819465, 'bagging_temperature': 0.433661621233731, 'border_count': 119}. Best is trial 2 with value: 0.6622717438107232.\n",
      "[I 2025-02-02 17:19:47,659] Trial 4 finished with value: 0.6412495047795538 and parameters: {'iterations': 301, 'depth': 6, 'learning_rate': 0.0013793784330126788, 'l2_leaf_reg': 4.463947616263535, 'random_strength': 0.3034853695017382, 'bagging_temperature': 0.9610380465730244, 'border_count': 122}. Best is trial 2 with value: 0.6622717438107232.\n",
      "[I 2025-02-02 17:20:42,510] Trial 5 finished with value: 0.6573947108048014 and parameters: {'iterations': 706, 'depth': 8, 'learning_rate': 0.07688034007270947, 'l2_leaf_reg': 3.195097140746614, 'random_strength': 2.6467362460745294, 'bagging_temperature': 0.9688890121365034, 'border_count': 232}. Best is trial 2 with value: 0.6622717438107232.\n",
      "[I 2025-02-02 17:22:23,382] Trial 6 finished with value: 0.6549948915045248 and parameters: {'iterations': 828, 'depth': 9, 'learning_rate': 0.003415543853364934, 'l2_leaf_reg': 2.0497277215387717, 'random_strength': 0.029365335702731355, 'bagging_temperature': 0.6899264019679918, 'border_count': 122}. Best is trial 2 with value: 0.6622717438107232.\n",
      "Number of finished trials: 7\n",
      "Best trial:\n",
      "  Best C-index: 0.6622717438107232\n",
      "  Params: \n",
      "    iterations: 719\n",
      "    depth: 6\n",
      "    learning_rate: 0.01884892696979274\n",
      "    l2_leaf_reg: 7.129540530307217\n",
      "    random_strength: 1.0704062122598603\n",
      "    bagging_temperature: 0.1596179759584443\n",
      "    border_count: 91\n",
      "0:\tlearn: 0.6904518\ttotal: 9.72ms\tremaining: 6.98s\n",
      "100:\tlearn: 0.6072063\ttotal: 967ms\tremaining: 5.91s\n",
      "200:\tlearn: 0.5906772\ttotal: 1.92s\tremaining: 4.94s\n",
      "300:\tlearn: 0.5816546\ttotal: 2.9s\tremaining: 4.03s\n",
      "400:\tlearn: 0.5748705\ttotal: 3.83s\tremaining: 3.04s\n",
      "500:\tlearn: 0.5688996\ttotal: 4.76s\tremaining: 2.07s\n",
      "600:\tlearn: 0.5628434\ttotal: 5.69s\tremaining: 1.12s\n",
      "700:\tlearn: 0.5565882\ttotal: 6.61s\tremaining: 170ms\n",
      "718:\tlearn: 0.5555049\ttotal: 6.78s\tremaining: 0us\n",
      "Submission file saved successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28800</td>\n",
       "      <td>0.169306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28801</td>\n",
       "      <td>0.644905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28802</td>\n",
       "      <td>0.077094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  prediction\n",
       "0  28800    0.169306\n",
       "1  28801    0.644905\n",
       "2  28802    0.077094"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### --- iPython Config --- ###\n",
    "from IPython import get_ipython\n",
    "if 'IPython.extensions.autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "else:\n",
    "    get_ipython().run_line_magic('reload_ext', 'autoreload')\n",
    "%autoreload 2\n",
    "### --- System and Path --- ###\n",
    "import os\n",
    "import sys\n",
    "repo_path = os.path.dirname(os.getcwd())\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.append(repo_path)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from lifelines.utils import concordance_index\n",
    "\n",
    "# Load data\n",
    "# local:\n",
    "df_train = pd.read_csv(os.path.join(repo_path, 'data', 'raw', 'train.csv'))\n",
    "df_test = pd.read_csv(os.path.join(repo_path, 'data', 'raw', 'test.csv'))\n",
    "# kaggle:\n",
    "# df_train = pd.read_csv('/kaggle/input/equity-post-HCT-survival-predictions/train.csv')\n",
    "# df_test = pd.read_csv('/kaggle/input/equity-post-HCT-survival-predictions/test.csv')\n",
    "\n",
    "# %%\n",
    "# drop columns\n",
    "cols = ['ID']\n",
    "id_df_test = df_test['ID']  # For submission\n",
    "df_train.drop(cols, axis=1, inplace=True)\n",
    "df_test.drop(cols, axis=1, inplace=True)\n",
    "\n",
    "# %%\n",
    "target = 'efs'\n",
    "time_col = 'efs_time'  # Survival time column\n",
    "\n",
    "X_train = df_train.drop([target, time_col], axis=1)\n",
    "y_train = df_train[target]\n",
    "efs_time = df_train[time_col]  # Extract survival times for C-index calculation\n",
    "\n",
    "X_test = df_test  # No target\n",
    "\n",
    "# %%\n",
    "# OneHotEncoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "cat_cols = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "num_cols = X_train.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "# Apply OneHotEncoding\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "X_train_cat = ohe.fit_transform(X_train[cat_cols])\n",
    "X_test_cat = ohe.transform(X_test[cat_cols])\n",
    "\n",
    "# Convert OHE to DataFrame and maintain column names\n",
    "X_train_cat = pd.DataFrame(X_train_cat, columns=ohe.get_feature_names_out(), index=X_train.index)\n",
    "X_test_cat = pd.DataFrame(X_test_cat, columns=ohe.get_feature_names_out(), index=X_test.index)\n",
    "\n",
    "# Concatenate with Numerical columns\n",
    "X_train_final = pd.concat([X_train[num_cols].reset_index(drop=True), X_train_cat.reset_index(drop=True)], axis=1)\n",
    "X_test_final = pd.concat([X_test[num_cols].reset_index(drop=True), X_test_cat.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Ensure the same feature set between train and test\n",
    "X_test_final = X_test_final.reindex(columns=X_train_final.columns, fill_value=0)\n",
    "\n",
    "# %%\n",
    "import optuna\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna optimization using C-index as the evaluation criterion.\n",
    "    \"\"\"\n",
    "    # --- 3.1. Suggest hyperparameters ---\n",
    "    param = {\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000),\n",
    "        'depth': trial.suggest_int('depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1.0, 10.0),\n",
    "        'random_strength': trial.suggest_float('random_strength', 1e-2, 10.0, log=True),\n",
    "        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n",
    "        'border_count': trial.suggest_int('border_count', 32, 256),\n",
    "        'verbose': 0,\n",
    "        'random_state': 42\n",
    "    }\n",
    "\n",
    "    # --- 3.2. Define the model ---\n",
    "    model = CatBoostClassifier(**param)\n",
    "\n",
    "    # --- 3.3. Evaluate using cross-validation ---\n",
    "    n_splits = 5\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    c_index_scores = []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X_train_final, y_train):\n",
    "        X_tr, X_val = X_train_final.iloc[train_idx], X_train_final.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        efs_time_val = efs_time.iloc[val_idx]  # Get survival times for validation\n",
    "\n",
    "        # Train model\n",
    "        model.fit(X_tr, y_tr, verbose=0)\n",
    "\n",
    "        # Predict risk scores (probabilities)\n",
    "        y_pred_proba = model.predict_proba(X_val)[:, 1]  # Higher values mean higher risk\n",
    "\n",
    "        # Compute Concordance Index\n",
    "        c_index = concordance_index(efs_time_val, -y_pred_proba, y_val)  # Negative for risk interpretation\n",
    "        c_index_scores.append(c_index)\n",
    "\n",
    "    return np.mean(c_index_scores)  # Maximizing mean C-index\n",
    "\n",
    "# Create Optuna study\n",
    "study = optuna.create_study(direction='maximize')  # We want to maximize C-index\n",
    "n_trials = 7\n",
    "study.optimize(objective, n_trials=n_trials, show_progress_bar=True)\n",
    "\n",
    "# Print results\n",
    "print(\"Number of finished trials:\", len(study.trials))\n",
    "print(\"Best trial:\")\n",
    "trial_ = study.best_trial\n",
    "\n",
    "print(\"  Best C-index:\", trial_.value)\n",
    "print(\"  Params: \")\n",
    "for key, value in trial_.params.items():\n",
    "    print(f\"    {key}: {value}\")\n",
    "\n",
    "# Train the final model with best parameters\n",
    "best_params = study.best_params\n",
    "final_model = CatBoostClassifier(\n",
    "    **best_params,\n",
    "    verbose=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "final_model.fit(X_train_final, y_train)\n",
    "\n",
    "# %%\n",
    "# Save the model (optional)\n",
    "# import joblib\n",
    "# joblib.dump(final_model, '../models/catboost_model.pkl')\n",
    "\n",
    "# %%\n",
    "# Inference\n",
    "y_pred_proba = final_model.predict_proba(X_test_final)[:, 1]  # Probability of event occurring\n",
    "\n",
    "# %%\n",
    "# Save submission\n",
    "submission = pd.DataFrame({'ID': id_df_test, 'prediction': y_pred_proba})\n",
    "# submission.to_csv(os.path.join(repo_path, 'data', 'submission', 'submission.csv'), index=False)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file saved successfully!\")\n",
    "submission"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10381525,
     "sourceId": 70942,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 244.250158,
   "end_time": "2025-02-02T17:22:31.352616",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-02T17:18:27.102458",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1256232a33fa4888a7f96230439e2c76": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2f70d247e98442a0bff0a9f4e651031a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f456bd28218b48ca8a0abb1a163306e5",
        "IPY_MODEL_348fb90e32534922ae866a89c1535cea",
        "IPY_MODEL_99c2aa12d63040d7b60448dc449b7cc6"
       ],
       "layout": "IPY_MODEL_a372d2ca55da443ab6e8aae9f29934f1",
       "tabbable": null,
       "tooltip": null
      }
     },
     "348fb90e32534922ae866a89c1535cea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_548558cadb21425fa3fef9305a927579",
       "max": 7,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3543bcb390944447adc4386a7e5ba51f",
       "tabbable": null,
       "tooltip": null,
       "value": 7
      }
     },
     "3543bcb390944447adc4386a7e5ba51f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3ca0c22c7fb941f783d6b92de7b73668": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "548558cadb21425fa3fef9305a927579": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "99c2aa12d63040d7b60448dc449b7cc6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1256232a33fa4888a7f96230439e2c76",
       "placeholder": "​",
       "style": "IPY_MODEL_bfe67bb1596d48cabb27d337b9657ef5",
       "tabbable": null,
       "tooltip": null,
       "value": " 7/7 [03:49&lt;00:00, 51.89s/it]"
      }
     },
     "9bd3cf423c1f4d308c5a9c8d7cb2de68": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a372d2ca55da443ab6e8aae9f29934f1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bfe67bb1596d48cabb27d337b9657ef5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f456bd28218b48ca8a0abb1a163306e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9bd3cf423c1f4d308c5a9c8d7cb2de68",
       "placeholder": "​",
       "style": "IPY_MODEL_3ca0c22c7fb941f783d6b92de7b73668",
       "tabbable": null,
       "tooltip": null,
       "value": "Best trial: 2. Best value: 0.662272: 100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
