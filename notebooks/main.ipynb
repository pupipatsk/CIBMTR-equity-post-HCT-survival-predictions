{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d734aef9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T17:18:29.952444Z",
     "iopub.status.busy": "2025-02-02T17:18:29.952010Z",
     "iopub.status.idle": "2025-02-02T17:18:29.984227Z",
     "shell.execute_reply": "2025-02-02T17:18:29.983025Z"
    },
    "papermill": {
     "duration": 0.038567,
     "end_time": "2025-02-02T17:18:29.986287",
     "exception": false,
     "start_time": "2025-02-02T17:18:29.947720",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ### Implementation of 'from lifelines.utils import concordance_index'\n",
    "# # because of no internet access(!pip install lifelines) when inferencing with kaggle\n",
    "# # src: https://github.com/CamDavidsonPilon/lifelines/blob/47afb1c1a272b0f03e0c8ca00e63df27eb2a0560/lifelines/utils/concordance.py\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "\n",
    "# class _BTree:\n",
    "\n",
    "#     \"\"\"A simple balanced binary order statistic tree to help compute the concordance.\n",
    "\n",
    "#     When computing the concordance, we know all the values the tree will ever contain. That\n",
    "#     condition simplifies this tree a lot. It means that instead of crazy AVL/red-black shenanigans\n",
    "#     we can simply do the following:\n",
    "\n",
    "#     - Store the final tree in flattened form in an array (so node i's children are 2i+1, 2i+2)\n",
    "#     - Additionally, store the current size of each subtree in another array with the same indices\n",
    "#     - To insert a value, just find its index, increment the size of the subtree at that index and\n",
    "#       propagate\n",
    "#     - To get the rank of an element, you add up a bunch of subtree counts\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, values):\n",
    "#         \"\"\"\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         values: list\n",
    "#             List of sorted (ascending), unique values that will be inserted.\n",
    "#         \"\"\"\n",
    "#         self._tree = self._treeify(values)\n",
    "#         self._counts = np.zeros_like(self._tree, dtype=int)\n",
    "\n",
    "#     @staticmethod\n",
    "#     def _treeify(values):\n",
    "#         \"\"\"Convert the np.ndarray `values` into a complete balanced tree.\n",
    "\n",
    "#         Assumes `values` is sorted ascending. Returns a list `t` of the same length in which t[i] >\n",
    "#         t[2i+1] and t[i] < t[2i+2] for all i.\"\"\"\n",
    "#         if len(values) == 1:  # this case causes problems later\n",
    "#             return values\n",
    "#         tree = np.empty_like(values)\n",
    "#         # Tree indices work as follows:\n",
    "#         # 0 is the root\n",
    "#         # 2n+1 is the left child of n\n",
    "#         # 2n+2 is the right child of n\n",
    "#         # So we now rearrange `values` into that format...\n",
    "\n",
    "#         # The first step is to remove the bottom row of leaves, which might not be exactly full\n",
    "#         last_full_row = int(np.log2(len(values) + 1) - 1)\n",
    "#         len_ragged_row = len(values) - (2 ** (last_full_row + 1) - 1)\n",
    "#         if len_ragged_row > 0:\n",
    "#             bottom_row_ix = np.s_[: 2 * len_ragged_row : 2]\n",
    "#             tree[-len_ragged_row:] = values[bottom_row_ix]\n",
    "#             values = np.delete(values, bottom_row_ix)\n",
    "\n",
    "#         # Now `values` is length 2**n - 1, so can be packed efficiently into a tree\n",
    "#         # Last row of nodes is indices 0, 2, ..., 2**n - 2\n",
    "#         # Second-last row is indices 1, 5, ..., 2**n - 3\n",
    "#         # nth-last row is indices (2**n - 1)::(2**(n+1))\n",
    "#         values_start = 0\n",
    "#         values_space = 2\n",
    "#         values_len = 2 ** last_full_row\n",
    "#         while values_start < len(values):\n",
    "#             tree[values_len - 1 : 2 * values_len - 1] = values[values_start::values_space]\n",
    "#             values_start += int(values_space / 2)\n",
    "#             values_space *= 2\n",
    "#             values_len = int(values_len / 2)\n",
    "#         return tree\n",
    "\n",
    "#     def insert(self, value):\n",
    "#         \"\"\"Insert an occurrence of `value` into the btree.\"\"\"\n",
    "#         i = 0\n",
    "#         n = len(self._tree)\n",
    "#         while i < n:\n",
    "#             cur = self._tree[i]\n",
    "#             self._counts[i] += 1\n",
    "#             if value < cur:\n",
    "#                 i = 2 * i + 1\n",
    "#             elif value > cur:\n",
    "#                 i = 2 * i + 2\n",
    "#             else:\n",
    "#                 return\n",
    "#         raise ValueError(\"Value %s not contained in tree.\" \"Also, the counts are now messed up.\" % value)\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return self._counts[0]\n",
    "\n",
    "#     def rank(self, value):\n",
    "#         \"\"\"Returns the rank and count of the value in the btree.\"\"\"\n",
    "#         i = 0\n",
    "#         n = len(self._tree)\n",
    "#         rank = 0\n",
    "#         count = 0\n",
    "#         while i < n:\n",
    "#             cur = self._tree[i]\n",
    "#             if value < cur:\n",
    "#                 i = 2 * i + 1\n",
    "#                 continue\n",
    "#             elif value > cur:\n",
    "#                 rank += self._counts[i]\n",
    "#                 # subtract off the right tree if exists\n",
    "#                 nexti = 2 * i + 2\n",
    "#                 if nexti < n:\n",
    "#                     rank -= self._counts[nexti]\n",
    "#                     i = nexti\n",
    "#                     continue\n",
    "#                 else:\n",
    "#                     return (rank, count)\n",
    "#             else:  # value == cur\n",
    "#                 count = self._counts[i]\n",
    "#                 lefti = 2 * i + 1\n",
    "#                 if lefti < n:\n",
    "#                     nleft = self._counts[lefti]\n",
    "#                     count -= nleft\n",
    "#                     rank += nleft\n",
    "#                     righti = lefti + 1\n",
    "#                     if righti < n:\n",
    "#                         count -= self._counts[righti]\n",
    "#                 return (rank, count)\n",
    "#         return (rank, count)\n",
    "\n",
    "# # -*- coding: utf-8 -*-\n",
    "\n",
    "# import numpy as np\n",
    "# # from lifelines.utils.btree import _BTree\n",
    "\n",
    "\n",
    "# def somers_d(event_times, x, event_observed=None) -> float:\n",
    "#     \"\"\"\n",
    "#     A measure of rank association between [-1, 1] between a censored variable, event_times,\n",
    "#     and another (uncensored) variable, x. -1 is strong anti-correlation, 1 is strong correlation.\n",
    "\n",
    "\n",
    "#     event_times: iterable\n",
    "#          a length-n iterable of observed survival times.\n",
    "#     x: iterable\n",
    "#         a length-n iterable to compare against\n",
    "#     event_observed: iterable, optional\n",
    "#         a length-n iterable censoring flags, 1 if observed, 0 if not. Default None assumes all observed.\n",
    "\n",
    "\n",
    "#     Examples\n",
    "#     --------\n",
    "#     .. code:: python\n",
    "#         from lifelines.datasets import load_rossi\n",
    "#         from lifelines.utils\n",
    "\n",
    "#         T, E = df['week'], df['arrest']\n",
    "#         x = df['age']\n",
    "#         somers_d(T, x, E)\n",
    "\n",
    "\n",
    "#     \"\"\"\n",
    "#     return 2 * concordance_index(event_times, x, event_observed) - 1\n",
    "\n",
    "\n",
    "# def concordance_index(event_times, predicted_scores, event_observed=None) -> float:\n",
    "#     \"\"\"\n",
    "#     Calculates the concordance index (C-index) between a series\n",
    "#     of event times and a predicted score. The first is the real survival times from\n",
    "#     the observational data, and the other is the predicted score from a model of some kind.\n",
    "\n",
    "#     The c-index is the average of how often a model says X is greater than Y when, in the observed\n",
    "#     data, X is indeed greater than Y. The c-index also handles how to handle censored values\n",
    "#     (obviously, if Y is censored, it's hard to know if X is truly greater than Y).\n",
    "\n",
    "\n",
    "#     The concordance index is a value between 0 and 1 where:\n",
    "\n",
    "#     - 0.5 is the expected result from random predictions,\n",
    "#     - 1.0 is perfect concordance and,\n",
    "#     - 0.0 is perfect anti-concordance (multiply predictions with -1 to get 1.0)\n",
    "\n",
    "#     The calculation internally done is\n",
    "\n",
    "#     >>> (pairs_correct + 0.5 * pairs_tied) / admissable_pairs\n",
    "\n",
    "#     where ``pairs_correct`` is the number of pairs s.t. if ``t_x > t_y``, then ``s_x > s_y``, pairs,\n",
    "#     ``pairs_tied`` is the number of pairs where ``s_x = s_y``, and ``admissable_pairs`` is all possible pairs. The subtleties\n",
    "#     are in how censored observation are handled (ex: not all pairs can be evaluated due to censoring).\n",
    "\n",
    "\n",
    "#     Parameters\n",
    "#     ----------\n",
    "#     event_times: iterable\n",
    "#          a length-n iterable of observed survival times.\n",
    "#     predicted_scores: iterable\n",
    "#         a length-n iterable of predicted scores - these could be survival times, or hazards, etc. See https://stats.stackexchange.com/questions/352183/use-median-survival-time-to-calculate-cph-c-statistic/352435#352435\n",
    "#     event_observed: iterable, optional\n",
    "#         a length-n iterable censoring flags, 1 if observed, 0 if not. Default None assumes all observed.\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#     c-index: float\n",
    "#       a value between 0 and 1.\n",
    "\n",
    "#     References\n",
    "#     -----------\n",
    "#     Harrell FE, Lee KL, Mark DB. Multivariable prognostic models: issues in\n",
    "#     developing models, evaluating assumptions and adequacy, and measuring and\n",
    "#     reducing errors. Statistics in Medicine 1996;15(4):361-87.\n",
    "\n",
    "#     Examples\n",
    "#     --------\n",
    "#     .. code:: python\n",
    "\n",
    "#         from lifelines.utils import concordance_index\n",
    "#         cph = CoxPHFitter().fit(df, 'T', 'E')\n",
    "#         concordance_index(df['T'], -cph.predict_partial_hazard(df), df['E'])\n",
    "\n",
    "#     \"\"\"\n",
    "#     event_times, predicted_scores, event_observed = _preprocess_scoring_data(event_times, predicted_scores, event_observed)\n",
    "#     num_correct, num_tied, num_pairs = _concordance_summary_statistics(event_times, predicted_scores, event_observed)\n",
    "\n",
    "#     return _concordance_ratio(num_correct, num_tied, num_pairs)\n",
    "\n",
    "\n",
    "# def _concordance_ratio(num_correct: int, num_tied: int, num_pairs: int) -> float:\n",
    "#     if num_pairs == 0:\n",
    "#         raise ZeroDivisionError(\"No admissable pairs in the dataset.\")\n",
    "#     return (num_correct + num_tied / 2) / num_pairs\n",
    "\n",
    "\n",
    "# def _concordance_summary_statistics(event_times, predicted_event_times, event_observed):  # pylint: disable=too-many-locals\n",
    "#     \"\"\"Find the concordance index in n * log(n) time.\n",
    "\n",
    "#     Assumes the data has been verified by lifelines.utils.concordance_index first.\n",
    "#     \"\"\"\n",
    "#     # Here's how this works.\n",
    "#     #\n",
    "#     # It would be pretty easy to do if we had no censored data and no ties. There, the basic idea\n",
    "#     # would be to iterate over the cases in order of their true event time (from least to greatest),\n",
    "#     # while keeping track of a pool of *predicted* event times for all cases previously seen (= all\n",
    "#     # cases that we know should be ranked lower than the case we're looking at currently).\n",
    "#     #\n",
    "#     # If the pool has O(log n) insert and O(log n) RANK (i.e., \"how many things in the pool have\n",
    "#     # value less than x\"), then the following algorithm is n log n:\n",
    "#     #\n",
    "#     # Sort the times and predictions by time, increasing\n",
    "#     # n_pairs, n_correct := 0\n",
    "#     # pool := {}\n",
    "#     # for each prediction p:\n",
    "#     #     n_pairs += len(pool)\n",
    "#     #     n_correct += rank(pool, p)\n",
    "#     #     add p to pool\n",
    "#     #\n",
    "#     # There are three complications: tied ground truth values, tied predictions, and censored\n",
    "#     # observations.\n",
    "#     #\n",
    "#     # - To handle tied true event times, we modify the inner loop to work in *batches* of observations\n",
    "#     # p_1, ..., p_n whose true event times are tied, and then add them all to the pool\n",
    "#     # simultaneously at the end.\n",
    "#     #\n",
    "#     # - To handle tied predictions, which should each count for 0.5, we switch to\n",
    "#     #     n_correct += min_rank(pool, p)\n",
    "#     #     n_tied += count(pool, p)\n",
    "#     #\n",
    "#     # - To handle censored observations, we handle each batch of tied, censored observations just\n",
    "#     # after the batch of observations that died at the same time (since those censored observations\n",
    "#     # are comparable all the observations that died at the same time or previously). However, we do\n",
    "#     # NOT add them to the pool at the end, because they are NOT comparable with any observations\n",
    "#     # that leave the study afterward--whether or not those observations get censored.\n",
    "#     if np.logical_not(event_observed).all():\n",
    "#         return (0, 0, 0)\n",
    "\n",
    "#     died_mask = event_observed.astype(bool)\n",
    "#     # TODO: is event_times already sorted? That would be nice...\n",
    "#     died_truth = event_times[died_mask]\n",
    "#     ix = np.argsort(died_truth)\n",
    "#     died_truth = died_truth[ix]\n",
    "#     died_pred = predicted_event_times[died_mask][ix]\n",
    "\n",
    "#     censored_truth = event_times[~died_mask]\n",
    "#     ix = np.argsort(censored_truth)\n",
    "#     censored_truth = censored_truth[ix]\n",
    "#     censored_pred = predicted_event_times[~died_mask][ix]\n",
    "\n",
    "#     censored_ix = 0\n",
    "#     died_ix = 0\n",
    "#     times_to_compare = _BTree(np.unique(died_pred))\n",
    "#     num_pairs = np.int64(0)\n",
    "#     num_correct = np.int64(0)\n",
    "#     num_tied = np.int64(0)\n",
    "\n",
    "#     # we iterate through cases sorted by exit time:\n",
    "#     # - First, all cases that died at time t0. We add these to the sortedlist of died times.\n",
    "#     # - Then, all cases that were censored at time t0. We DON'T add these since they are NOT\n",
    "#     #   comparable to subsequent elements.\n",
    "#     while True:\n",
    "#         has_more_censored = censored_ix < len(censored_truth)\n",
    "#         has_more_died = died_ix < len(died_truth)\n",
    "#         # Should we look at some censored indices next, or died indices?\n",
    "#         if has_more_censored and (not has_more_died or died_truth[died_ix] > censored_truth[censored_ix]):\n",
    "#             pairs, correct, tied, next_ix = _handle_pairs(censored_truth, censored_pred, censored_ix, times_to_compare)\n",
    "#             censored_ix = next_ix\n",
    "#         elif has_more_died and (not has_more_censored or died_truth[died_ix] <= censored_truth[censored_ix]):\n",
    "#             pairs, correct, tied, next_ix = _handle_pairs(died_truth, died_pred, died_ix, times_to_compare)\n",
    "#             for pred in died_pred[died_ix:next_ix]:\n",
    "#                 times_to_compare.insert(pred)\n",
    "#             died_ix = next_ix\n",
    "#         else:\n",
    "#             assert not (has_more_died or has_more_censored)\n",
    "#             break\n",
    "\n",
    "#         num_pairs += pairs\n",
    "#         num_correct += correct\n",
    "#         num_tied += tied\n",
    "\n",
    "#     return (num_correct, num_tied, num_pairs)\n",
    "\n",
    "\n",
    "# def _handle_pairs(truth, pred, first_ix, times_to_compare):\n",
    "#     \"\"\"\n",
    "#     Handle all pairs that exited at the same time as truth[first_ix].\n",
    "\n",
    "#     Returns\n",
    "#     -------\n",
    "#       (pairs, correct, tied, next_ix)\n",
    "#       new_pairs: The number of new comparisons performed\n",
    "#       new_correct: The number of comparisons correctly predicted\n",
    "#       next_ix: The next index that needs to be handled\n",
    "#     \"\"\"\n",
    "#     next_ix = first_ix\n",
    "#     while next_ix < len(truth) and truth[next_ix] == truth[first_ix]:\n",
    "#         next_ix += 1\n",
    "#     pairs = len(times_to_compare) * (next_ix - first_ix)\n",
    "#     correct = np.int64(0)\n",
    "#     tied = np.int64(0)\n",
    "#     for i in range(first_ix, next_ix):\n",
    "#         rank, count = times_to_compare.rank(pred[i])\n",
    "#         correct += rank\n",
    "#         tied += count\n",
    "\n",
    "#     return (pairs, correct, tied, next_ix)\n",
    "\n",
    "\n",
    "# def _naive_concordance_summary_statistics(event_times, predicted_event_times, event_observed):\n",
    "#     \"\"\"\n",
    "#     Fallback, simpler method to compute concordance.\n",
    "\n",
    "#     \"\"\"\n",
    "\n",
    "#     def _valid_comparison(time_a, time_b, event_a, event_b):\n",
    "#         \"\"\"True if times can be compared.\"\"\"\n",
    "#         if time_a == time_b:\n",
    "#             # Ties are only informative if exactly one event happened\n",
    "#             return event_a != event_b\n",
    "#         if event_a and event_b:\n",
    "#             return True\n",
    "#         if event_a and time_a < time_b:\n",
    "#             return True\n",
    "#         if event_b and time_b < time_a:\n",
    "#             return True\n",
    "#         return False\n",
    "\n",
    "#     def _concordance_value(time_a, time_b, pred_a, pred_b, event_a, event_b):\n",
    "#         if pred_a == pred_b:\n",
    "#             # Same as random\n",
    "#             return (0, 1)\n",
    "#         if pred_a < pred_b:\n",
    "#             return (time_a < time_b) or (time_a == time_b and event_a and not event_b), 0\n",
    "#         # pred_a > pred_b\n",
    "#         return (time_a > time_b) or (time_a == time_b and not event_a and event_b), 0\n",
    "\n",
    "#     num_pairs = 0.0\n",
    "#     num_correct = 0.0\n",
    "#     num_tied = 0.0\n",
    "\n",
    "#     for a, time_a in enumerate(event_times):\n",
    "#         pred_a = predicted_event_times[a]\n",
    "#         event_a = event_observed[a]\n",
    "#         # Don't want to double count\n",
    "#         for b in range(a + 1, len(event_times)):\n",
    "#             time_b = event_times[b]\n",
    "#             pred_b = predicted_event_times[b]\n",
    "#             event_b = event_observed[b]\n",
    "\n",
    "#             if _valid_comparison(time_a, time_b, event_a, event_b):\n",
    "#                 num_pairs += 1.0\n",
    "#                 crct, ties = _concordance_value(time_a, time_b, pred_a, pred_b, event_a, event_b)\n",
    "#                 num_correct += crct\n",
    "#                 num_tied += ties\n",
    "\n",
    "#     return (num_correct, num_tied, num_pairs)\n",
    "\n",
    "\n",
    "# def naive_concordance_index(event_times, predicted_event_times, event_observed=None) -> float:\n",
    "#     event_times, predicted_event_times, event_observed = _preprocess_scoring_data(\n",
    "#         event_times, predicted_event_times, event_observed\n",
    "#     )\n",
    "#     return _concordance_ratio(*_naive_concordance_summary_statistics(event_times, predicted_event_times, event_observed))\n",
    "\n",
    "\n",
    "# def _preprocess_scoring_data(event_times, predicted_scores, event_observed):\n",
    "#     event_times = np.asarray(event_times, dtype=float)\n",
    "#     predicted_scores = np.asarray(predicted_scores, dtype=float)\n",
    "\n",
    "#     # Allow for (n, 1) or (1, n) arrays\n",
    "#     if event_times.ndim == 2 and (event_times.shape[0] == 1 or event_times.shape[1] == 1):\n",
    "#         # Flatten array\n",
    "#         event_times = event_times.ravel()\n",
    "#     # Allow for (n, 1) or (1, n) arrays\n",
    "#     if predicted_scores.ndim == 2 and (predicted_scores.shape[0] == 1 or predicted_scores.shape[1] == 1):\n",
    "#         # Flatten array\n",
    "#         predicted_scores = predicted_scores.ravel()\n",
    "\n",
    "#     if event_times.shape != predicted_scores.shape:\n",
    "#         raise ValueError(\"Event times and predictions must have the same shape\")\n",
    "#     if event_times.ndim != 1:\n",
    "#         raise ValueError(\"Event times can only be 1-dimensional: (n,)\")\n",
    "\n",
    "#     if event_observed is None:\n",
    "#         event_observed = np.ones(event_times.shape[0], dtype=float)\n",
    "#     else:\n",
    "#         event_observed = np.asarray(event_observed, dtype=float).ravel()\n",
    "#         if event_observed.shape != event_times.shape:\n",
    "#             raise ValueError(\"Observed events must be 1-dimensional of same length as event times\")\n",
    "\n",
    "#     # check for NaNs\n",
    "#     for a in [event_times, predicted_scores, event_observed]:\n",
    "#         if np.isnan(a).any():\n",
    "#             raise ValueError(\"NaNs detected in inputs, please correct or drop.\")\n",
    "\n",
    "#     return event_times, predicted_scores, event_observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "628cb5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "### --- iPython Config --- ###\n",
    "from IPython import get_ipython\n",
    "if 'IPython.extensions.autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    get_ipython().run_line_magic('load_ext', 'autoreload')\n",
    "else:\n",
    "    get_ipython().run_line_magic('reload_ext', 'autoreload')\n",
    "%autoreload 2\n",
    "### --- System and Path --- ###\n",
    "import os\n",
    "import sys\n",
    "repo_path = os.path.dirname(os.getcwd())\n",
    "if repo_path not in sys.path:\n",
    "    sys.path.append(repo_path)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "from lifelines.utils import concordance_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb19353a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KERNEL_RUN_TYPE: local\n"
     ]
    }
   ],
   "source": [
    "# Configs\n",
    "SEED = 42\n",
    "kernel_run_type = \"kaggle\" if \"KAGGLE_KERNEL_RUN_TYPE\" in os.environ else \"local\"\n",
    "print(f\"KERNEL_RUN_TYPE: {kernel_run_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efa56f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "if kernel_run_type==\"kaggle\":\n",
    "    df_train = pd.read_csv('/kaggle/input/equity-post-HCT-survival-predictions/train.csv')\n",
    "    df_test = pd.read_csv('/kaggle/input/equity-post-HCT-survival-predictions/test.csv')\n",
    "else:\n",
    "    # local\n",
    "    df_train = pd.read_csv(os.path.join(repo_path, 'data', 'raw', 'train.csv'))\n",
    "    df_test = pd.read_csv(os.path.join(repo_path, 'data', 'raw', 'test.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe397ca8",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16b54be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "cols = ['ID']\n",
    "id_df_test = df_test['ID']  # For submission\n",
    "df_train.drop(cols, axis=1, inplace=True)\n",
    "df_test.drop(cols, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "286472c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns\n",
    "categorical_cols = df_train.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = df_train.select_dtypes(include=['number']).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e1b8f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "target = 'efs'\n",
    "target_time = 'efs_time'  # Survival time column\n",
    "numerical_cols = [col for col in numerical_cols if col not in [target, target_time]]\n",
    "# train dataset\n",
    "X_train = df_train.drop([target, target_time], axis=1)\n",
    "y_train = df_train[target]\n",
    "efs_time = df_train[target_time]  # Extract survival times for C-index calculation\n",
    "# test dataset\n",
    "X_test = df_test  # No target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36500336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def one_hot_encoding(X_train, X_test, cat_cols=None):\n",
    "    # Identify categorical and numerical columns\n",
    "    num_cols = X_train.columns.difference(cat_cols)\n",
    "\n",
    "    # Apply OneHotEncoding\n",
    "    ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
    "    X_train_cat = ohe.fit_transform(X_train[cat_cols])\n",
    "    X_test_cat = ohe.transform(X_test[cat_cols])\n",
    "\n",
    "    # Convert OHE to DataFrame and maintain column names\n",
    "    X_train_cat = pd.DataFrame(X_train_cat, columns=ohe.get_feature_names_out(), index=X_train.index)\n",
    "    X_test_cat = pd.DataFrame(X_test_cat, columns=ohe.get_feature_names_out(), index=X_test.index)\n",
    "\n",
    "    # Concatenate with Numerical columns\n",
    "    X_train_final = pd.concat([X_train[num_cols].reset_index(drop=True), X_train_cat.reset_index(drop=True)], axis=1)\n",
    "    X_test_final = pd.concat([X_test[num_cols].reset_index(drop=True), X_test_cat.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # Ensure the same feature set between train and test\n",
    "    X_test_final = X_test_final.reindex(columns=X_train_final.columns, fill_value=0)\n",
    "\n",
    "    return X_train_final, X_test_final\n",
    "\n",
    "X_train, X_test = one_hot_encoding(X_train.copy(), X_test.copy(), cat_cols=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "554d89d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Transforms numerical features into a Gaussian distribution using QuantileTransformer\n",
    "# from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "# def transform_to_gaussian(X_train, X_test, num_cols=None):\n",
    "#     \"\"\"\n",
    "#     Transforms numerical features into a Gaussian distribution using QuantileTransformer.\n",
    "\n",
    "#     Parameters:\n",
    "#     - X_train: DataFrame of training features.\n",
    "#     - X_test: DataFrame of test features.\n",
    "\n",
    "#     Returns:\n",
    "#     - X_train_transformed: Transformed training data.\n",
    "#     - X_test_transformed: Transformed test data.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Apply Quantile Transformation\n",
    "#     qt = QuantileTransformer(output_distribution=\"normal\", random_state=42)\n",
    "#     X_train_gauss = qt.fit_transform(X_train[num_cols])\n",
    "#     X_test_gauss = qt.transform(X_test[num_cols])\n",
    "\n",
    "#     # Convert back to DataFrame\n",
    "#     X_train_gauss = pd.DataFrame(X_train_gauss, columns=num_cols, index=X_train.index)\n",
    "#     X_test_gauss = pd.DataFrame(X_test_gauss, columns=num_cols, index=X_test.index)\n",
    "\n",
    "#     # Replace original numerical features with transformed ones\n",
    "#     X_train_final = X_train.copy()\n",
    "#     X_test_final = X_test.copy()\n",
    "#     X_train_final[num_cols] = X_train_gauss\n",
    "#     X_test_final[num_cols] = X_test_gauss\n",
    "\n",
    "#     return X_train_final, X_test_final\n",
    "\n",
    "# # Apply the transformation\n",
    "# X_train, X_test = transform_to_gaussian(X_train.copy(), X_test.copy(), num_cols=numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15647c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scale_to_standard(X_train, X_test, num_cols=None):\n",
    "    \"\"\"\n",
    "    Scales numerical features to standard normal distribution (mean=0, std=1).\n",
    "\n",
    "    Parameters:\n",
    "    - X_train: DataFrame of training features.\n",
    "    - X_test: DataFrame of test features.\n",
    "\n",
    "    Returns:\n",
    "    - X_train_scaled: Scaled training data.\n",
    "    - X_test_scaled: Scaled test data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply Standard Scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train[num_cols])\n",
    "    X_test_scaled = scaler.transform(X_test[num_cols])\n",
    "\n",
    "    # Convert back to DataFrame\n",
    "    X_train_scaled = pd.DataFrame(X_train_scaled, columns=num_cols, index=X_train.index)\n",
    "    X_test_scaled = pd.DataFrame(X_test_scaled, columns=num_cols, index=X_test.index)\n",
    "\n",
    "    # Replace original numerical features with scaled ones\n",
    "    X_train_final = X_train.copy()\n",
    "    X_test_final = X_test.copy()\n",
    "    X_train_final[num_cols] = X_train_scaled\n",
    "    X_test_final[num_cols] = X_test_scaled\n",
    "\n",
    "    return X_train_final, X_test_final\n",
    "\n",
    "# Apply the transformation\n",
    "X_train, X_test = scale_to_standard(X_train.copy(), X_test.copy(), num_cols=numerical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36e5d0c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5231361",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-02T17:18:29.992554Z",
     "iopub.status.busy": "2025-02-02T17:18:29.992068Z",
     "iopub.status.idle": "2025-02-02T17:22:30.524401Z",
     "shell.execute_reply": "2025-02-02T17:22:30.523306Z"
    },
    "papermill": {
     "duration": 240.537606,
     "end_time": "2025-02-02T17:22:30.526545",
     "exception": false,
     "start_time": "2025-02-02T17:18:29.988939",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-02-05 16:40:49,493] A new study created in memory with name: no-name-2e9e4cde-3b8b-4885-99f6-0a56b9156d6f\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2801850d070740cda03b47342e92f566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting cross-validation for trial: 0\n",
      "0:\tlearn: 0.6913826\ttotal: 64.9ms\tremaining: 10m 49s\n",
      "2000:\tlearn: 0.4162544\ttotal: 14.1s\tremaining: 56.5s\n",
      "4000:\tlearn: 0.3114298\ttotal: 28.4s\tremaining: 42.5s\n",
      "6000:\tlearn: 0.2383780\ttotal: 42.6s\tremaining: 28.4s\n",
      "8000:\tlearn: 0.1853557\ttotal: 56.5s\tremaining: 14.1s\n",
      "9999:\tlearn: 0.1453708\ttotal: 1m 10s\tremaining: 0us\n",
      "Trial 0, Fold 1: C-Index=0.6470388220098023\n",
      "0:\tlearn: 0.6912803\ttotal: 7.03ms\tremaining: 1m 10s\n",
      "2000:\tlearn: 0.4221408\ttotal: 13.8s\tremaining: 55s\n",
      "4000:\tlearn: 0.3164338\ttotal: 27.5s\tremaining: 41.2s\n",
      "6000:\tlearn: 0.2424095\ttotal: 41.2s\tremaining: 27.4s\n",
      "8000:\tlearn: 0.1881139\ttotal: 54.9s\tremaining: 13.7s\n",
      "9999:\tlearn: 0.1484385\ttotal: 1m 8s\tremaining: 0us\n",
      "Trial 0, Fold 2: C-Index=0.6598219790933118\n",
      "0:\tlearn: 0.6912804\ttotal: 7.19ms\tremaining: 1m 11s\n",
      "2000:\tlearn: 0.4224437\ttotal: 13.7s\tremaining: 54.8s\n",
      "4000:\tlearn: 0.3166128\ttotal: 27.4s\tremaining: 41s\n",
      "6000:\tlearn: 0.2435163\ttotal: 41.1s\tremaining: 27.4s\n",
      "8000:\tlearn: 0.1903288\ttotal: 54.8s\tremaining: 13.7s\n",
      "9999:\tlearn: 0.1500420\ttotal: 1m 9s\tremaining: 0us\n",
      "Trial 0, Fold 3: C-Index=0.6627980369449914\n",
      "0:\tlearn: 0.6912825\ttotal: 7.48ms\tremaining: 1m 14s\n",
      "2000:\tlearn: 0.4221333\ttotal: 13.7s\tremaining: 54.8s\n",
      "4000:\tlearn: 0.3162381\ttotal: 27.3s\tremaining: 41s\n",
      "6000:\tlearn: 0.2433255\ttotal: 41s\tremaining: 27.3s\n",
      "8000:\tlearn: 0.1898124\ttotal: 54.9s\tremaining: 13.7s\n",
      "9999:\tlearn: 0.1500232\ttotal: 1m 9s\tremaining: 0us\n",
      "Trial 0, Fold 4: C-Index=0.6632066529279254\n",
      "0:\tlearn: 0.6912639\ttotal: 7.01ms\tremaining: 1m 10s\n",
      "2000:\tlearn: 0.4196904\ttotal: 14.5s\tremaining: 58.1s\n",
      "4000:\tlearn: 0.3127684\ttotal: 28.1s\tremaining: 42.1s\n",
      "6000:\tlearn: 0.2392499\ttotal: 41.6s\tremaining: 27.7s\n",
      "8000:\tlearn: 0.1859227\ttotal: 56s\tremaining: 14s\n",
      "9999:\tlearn: 0.1464729\ttotal: 1m 11s\tremaining: 0us\n",
      "Trial 0, Fold 5: C-Index=0.6506303143946228\n",
      "[I 2025-02-05 16:46:42,297] Trial 0 finished with value: 0.6566991610741307 and parameters: {'learning_rate': 0.012693517296473746, 'depth': 8, 'l2_leaf_reg': 2.2328789280464627}. Best is trial 0 with value: 0.6566991610741307.\n",
      "Starting cross-validation for trial: 1\n",
      "0:\tlearn: 0.6738823\ttotal: 3.96ms\tremaining: 39.6s\n",
      "2000:\tlearn: 0.2408316\ttotal: 7.83s\tremaining: 31.3s\n",
      "4000:\tlearn: 0.1218501\ttotal: 15.6s\tremaining: 23.4s\n",
      "6000:\tlearn: 0.0665448\ttotal: 23.4s\tremaining: 15.6s\n",
      "8000:\tlearn: 0.0391067\ttotal: 31.3s\tremaining: 7.83s\n",
      "9999:\tlearn: 0.0258320\ttotal: 38.5s\tremaining: 0us\n",
      "Trial 1, Fold 1: C-Index=0.6268577089048595\n",
      "0:\tlearn: 0.6732391\ttotal: 4.33ms\tremaining: 43.3s\n",
      "2000:\tlearn: 0.2476785\ttotal: 7.25s\tremaining: 29s\n",
      "4000:\tlearn: 0.1253929\ttotal: 14.6s\tremaining: 21.9s\n",
      "6000:\tlearn: 0.0683144\ttotal: 22s\tremaining: 14.7s\n",
      "8000:\tlearn: 0.0404071\ttotal: 29.3s\tremaining: 7.33s\n",
      "9999:\tlearn: 0.0266978\ttotal: 36.4s\tremaining: 0us\n",
      "Trial 1, Fold 2: C-Index=0.6409482826868573\n",
      "0:\tlearn: 0.6731139\ttotal: 3.85ms\tremaining: 38.5s\n",
      "2000:\tlearn: 0.2469857\ttotal: 7.3s\tremaining: 29.2s\n",
      "4000:\tlearn: 0.1258539\ttotal: 14.6s\tremaining: 21.9s\n",
      "6000:\tlearn: 0.0693062\ttotal: 21.9s\tremaining: 14.6s\n",
      "8000:\tlearn: 0.0410618\ttotal: 29.2s\tremaining: 7.29s\n",
      "9999:\tlearn: 0.0268900\ttotal: 36.6s\tremaining: 0us\n",
      "Trial 1, Fold 3: C-Index=0.6458961880057271\n",
      "0:\tlearn: 0.6728592\ttotal: 5.2ms\tremaining: 52s\n",
      "2000:\tlearn: 0.2490459\ttotal: 7.41s\tremaining: 29.6s\n",
      "4000:\tlearn: 0.1271758\ttotal: 14.8s\tremaining: 22.2s\n",
      "6000:\tlearn: 0.0701878\ttotal: 22.1s\tremaining: 14.7s\n",
      "8000:\tlearn: 0.0415852\ttotal: 29.5s\tremaining: 7.36s\n",
      "9999:\tlearn: 0.0272448\ttotal: 36.6s\tremaining: 0us\n",
      "Trial 1, Fold 4: C-Index=0.64114576497542\n",
      "0:\tlearn: 0.6729593\ttotal: 3.95ms\tremaining: 39.5s\n",
      "2000:\tlearn: 0.2447506\ttotal: 7.29s\tremaining: 29.1s\n",
      "4000:\tlearn: 0.1241384\ttotal: 14.6s\tremaining: 21.9s\n",
      "6000:\tlearn: 0.0677051\ttotal: 21.9s\tremaining: 14.6s\n",
      "8000:\tlearn: 0.0400984\ttotal: 29.2s\tremaining: 7.29s\n",
      "9999:\tlearn: 0.0264204\ttotal: 36.3s\tremaining: 0us\n",
      "Trial 1, Fold 5: C-Index=0.6309161728591114\n",
      "[I 2025-02-05 16:49:48,947] Trial 1 finished with value: 0.6371528234863951 and parameters: {'learning_rate': 0.17217395893888982, 'depth': 5, 'l2_leaf_reg': 3.6815797643502908}. Best is trial 0 with value: 0.6566991610741307.\n",
      "Starting cross-validation for trial: 2\n",
      "0:\tlearn: 0.6920399\ttotal: 4.15ms\tremaining: 41.5s\n",
      "2000:\tlearn: 0.5472764\ttotal: 7.25s\tremaining: 29s\n",
      "4000:\tlearn: 0.5080693\ttotal: 14.5s\tremaining: 21.7s\n",
      "6000:\tlearn: 0.4770712\ttotal: 21.7s\tremaining: 14.5s\n",
      "8000:\tlearn: 0.4505630\ttotal: 29.2s\tremaining: 7.3s\n",
      "9999:\tlearn: 0.4268985\ttotal: 36.8s\tremaining: 0us\n",
      "Trial 2, Fold 1: C-Index=0.6538098446810843\n",
      "0:\tlearn: 0.6919968\ttotal: 4.11ms\tremaining: 41.1s\n",
      "2000:\tlearn: 0.5530918\ttotal: 7.35s\tremaining: 29.4s\n",
      "4000:\tlearn: 0.5141443\ttotal: 14.6s\tremaining: 21.9s\n",
      "6000:\tlearn: 0.4837242\ttotal: 22.2s\tremaining: 14.8s\n",
      "8000:\tlearn: 0.4572020\ttotal: 29.8s\tremaining: 7.44s\n",
      "9999:\tlearn: 0.4335665\ttotal: 37.2s\tremaining: 0us\n",
      "Trial 2, Fold 2: C-Index=0.6680999835362251\n",
      "0:\tlearn: 0.6919904\ttotal: 3.85ms\tremaining: 38.5s\n",
      "2000:\tlearn: 0.5527626\ttotal: 7.33s\tremaining: 29.3s\n",
      "4000:\tlearn: 0.5133132\ttotal: 15.8s\tremaining: 23.6s\n",
      "6000:\tlearn: 0.4827010\ttotal: 23.4s\tremaining: 15.6s\n",
      "8000:\tlearn: 0.4565966\ttotal: 30.9s\tremaining: 7.73s\n",
      "9999:\tlearn: 0.4331695\ttotal: 38.6s\tremaining: 0us\n",
      "Trial 2, Fold 3: C-Index=0.667316200150014\n",
      "0:\tlearn: 0.6919784\ttotal: 4.5ms\tremaining: 45s\n",
      "2000:\tlearn: 0.5523401\ttotal: 7.61s\tremaining: 30.4s\n",
      "4000:\tlearn: 0.5134867\ttotal: 15.2s\tremaining: 22.8s\n",
      "6000:\tlearn: 0.4828693\ttotal: 22.6s\tremaining: 15s\n",
      "8000:\tlearn: 0.4566619\ttotal: 30s\tremaining: 7.5s\n",
      "9999:\tlearn: 0.4335804\ttotal: 37.8s\tremaining: 0us\n",
      "Trial 2, Fold 4: C-Index=0.6677967740821331\n",
      "0:\tlearn: 0.6919813\ttotal: 4ms\tremaining: 40s\n",
      "2000:\tlearn: 0.5506294\ttotal: 7.48s\tremaining: 29.9s\n",
      "4000:\tlearn: 0.5109510\ttotal: 15.4s\tremaining: 23s\n",
      "6000:\tlearn: 0.4795648\ttotal: 23.2s\tremaining: 15.5s\n",
      "8000:\tlearn: 0.4531161\ttotal: 30.8s\tremaining: 7.69s\n",
      "9999:\tlearn: 0.4295259\ttotal: 38.2s\tremaining: 0us\n",
      "Trial 2, Fold 5: C-Index=0.6569436950119194\n",
      "[I 2025-02-05 16:52:59,672] Trial 2 finished with value: 0.6627932994922752 and parameters: {'learning_rate': 0.008985951049581406, 'depth': 5, 'l2_leaf_reg': 1.1106432402765392}. Best is trial 2 with value: 0.6627932994922752.\n",
      "Number of finished trials: 3\n",
      "Best trial:\n",
      "  Best C-index: 0.6627932994922752\n",
      "  Params: \n",
      "    learning_rate: 0.008985951049581406\n",
      "    depth: 5\n",
      "    l2_leaf_reg: 1.1106432402765392\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "import os\n",
    "from lifelines.utils import concordance_index\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute class weights to handle imbalanced target\n",
    "def compute_scale_pos_weight(y_train):\n",
    "    unique_classes = np.unique(y_train)\n",
    "    if len(unique_classes) < 2:\n",
    "        return 1  # Avoid issues if only one class is present (shouldn't happen in normal training)\n",
    "\n",
    "    class_weights = compute_class_weight(\"balanced\", classes=unique_classes, y=y_train)\n",
    "    neg_class_weight = class_weights[0]  # Weight for class 0\n",
    "    pos_class_weight = class_weights[1]  # Weight for class 1\n",
    "\n",
    "    return (\n",
    "        pos_class_weight / neg_class_weight\n",
    "    )  # Scale positive class relative to negative\n",
    "scale_pos_weight = compute_scale_pos_weight(y_train)\n",
    "\n",
    "# Parameters\n",
    "DEFAULT_ITERATIONS = 10_000\n",
    "VERBOSE = 2_000\n",
    "train_dir = os.path.join(repo_path, \"models\", \"catboost_info\")\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "base_params = {\n",
    "    \"loss_function\": \"Logloss\",\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"train_dir\": train_dir,\n",
    "    \"iterations\": DEFAULT_ITERATIONS,\n",
    "    \"verbose\": VERBOSE,\n",
    "    \"random_state\": SEED,\n",
    "}\n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna optimization using C-index as the evaluation criterion.\n",
    "    \"\"\"\n",
    "\n",
    "    learnable_params = {\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 5e-3, 2e-1),\n",
    "        \"depth\": trial.suggest_int(\"depth\", 5, 8),\n",
    "        \"l2_leaf_reg\": trial.suggest_uniform(\"l2_leaf_reg\", 1, 5),\n",
    "    }\n",
    "\n",
    "    params = {**base_params, **learnable_params}\n",
    "    model = CatBoostClassifier(**params)\n",
    "\n",
    "    # --- 3.3. Evaluate using cross-validation ---\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "    c_index_scores = []\n",
    "    print(f\"Starting cross-validation for trial: {trial.number+1}\")\n",
    "    for fold, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train), 1):\n",
    "        # print(f\"Trial {trial.number}, Fold {fold}: Training the model...\")\n",
    "\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        efs_time_val = efs_time.iloc[val_idx]  # Get survival times for validation\n",
    "\n",
    "        # Train model\n",
    "        model.fit(X_tr, y_tr)\n",
    "\n",
    "        # Predict risk scores (probabilities)\n",
    "        y_pred_proba = model.predict_proba(X_val)[:, 1]  # Higher values mean higher risk\n",
    "\n",
    "        # Compute Concordance Index\n",
    "        c_index = concordance_index(\n",
    "            efs_time_val, -y_pred_proba, y_val\n",
    "        )  # Negative for risk interpretation\n",
    "        print(f\"Trial {trial.number+1}, Fold {fold}: C-Index={c_index}\")\n",
    "        c_index_scores.append(c_index)\n",
    "\n",
    "    return np.mean(c_index_scores)  # Maximizing mean C-index\n",
    "\n",
    "# Print results\n",
    "def print_optuna_results(study):\n",
    "    print(\"Number of finished trials:\", len(study.trials))\n",
    "    print(\"Best trial:\")\n",
    "    trial_ = study.best_trial\n",
    "    print(\"  Best C-index:\", trial_.value)\n",
    "    print(\"  Params: \")\n",
    "    for key, value in trial_.params.items():\n",
    "        print(f\"    {key}: {value}\")\n",
    "\n",
    "\n",
    "# Running the optimization...\n",
    "n_trials = 3\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=n_trials, show_progress_bar=True, timeout=60*10)\n",
    "print_optuna_results(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42d4e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6919783\ttotal: 4.46ms\tremaining: 44.6s\n",
      "2000:\tlearn: 0.5570209\ttotal: 8.57s\tremaining: 34.3s\n",
      "4000:\tlearn: 0.5233932\ttotal: 17.1s\tremaining: 25.6s\n",
      "6000:\tlearn: 0.4969145\ttotal: 25.6s\tremaining: 17.1s\n",
      "8000:\tlearn: 0.4740940\ttotal: 34.5s\tremaining: 8.61s\n",
      "9999:\tlearn: 0.4536640\ttotal: 43s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/pupipatsingkhorn/Developer/repositories/CIBMTR-equity-post-HCT-survival-predictions/models/20250205-1653-model.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the final model with best parameters\n",
    "best_learnable_params = study.best_params\n",
    "params = {**base_params, **best_learnable_params}\n",
    "final_model = CatBoostClassifier(**params)\n",
    "\n",
    "# Fitting final model...\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model\n",
    "if kernel_run_type==\"kaggle\":\n",
    "    filepath = ''\n",
    "else:\n",
    "    time_now = time.strftime(\"%Y%m%d-%H%M\")\n",
    "    filepath = os.path.join(repo_path, \"models\", f\"{time_now}-model.pkl\")\n",
    "joblib.dump(final_model, filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96ed8fa",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d2581ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load final model\n",
    "if kernel_run_type==\"kaggle\":\n",
    "    # change here\n",
    "    filepath = '/kaggle/input/20250204-2157-model.pkl/scikitlearn/default/1/20250204-2157-model.pkl'\n",
    "else: # local\n",
    "    model_name = \"20250204-2157-model.pkl\" # change here\n",
    "    filepath = os.path.join(repo_path, \"models\", model_name)\n",
    "final_model = joblib.load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54fa4534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission saved on local successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28800</td>\n",
       "      <td>0.139553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28801</td>\n",
       "      <td>0.634374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28802</td>\n",
       "      <td>0.028392</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  prediction\n",
       "0  28800    0.139553\n",
       "1  28801    0.634374\n",
       "2  28802    0.028392"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inference\n",
    "y_pred_proba = final_model.predict_proba(X_test)[:, 1]  # Probability of event occurring\n",
    "\n",
    "# Save submission\n",
    "df_submission = pd.DataFrame({'ID': id_df_test, 'prediction': y_pred_proba})\n",
    "if kernel_run_type==\"kaggle\":\n",
    "    df_submission.to_csv('submission.csv', index=False)\n",
    "else:\n",
    "    # local\n",
    "    # main file:\n",
    "    filepath = os.path.join(repo_path, 'data', 'submission', 'submission.csv')\n",
    "    df_submission.to_csv(filepath, index=False)\n",
    "    # archive file:\n",
    "    time_now = time.strftime(\"%Y%m%d-%H%M\")\n",
    "    filepath = os.path.join(repo_path, 'data', 'submission', 'archive', f'{time_now}-submission.csv')\n",
    "    df_submission.to_csv(filepath, index=False)\n",
    "print(f\"Submission saved on {kernel_run_type} successfully.\")\n",
    "\n",
    "df_submission"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10381525,
     "sourceId": 70942,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "datascience",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 244.250158,
   "end_time": "2025-02-02T17:22:31.352616",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-02-02T17:18:27.102458",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "1256232a33fa4888a7f96230439e2c76": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2f70d247e98442a0bff0a9f4e651031a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_f456bd28218b48ca8a0abb1a163306e5",
        "IPY_MODEL_348fb90e32534922ae866a89c1535cea",
        "IPY_MODEL_99c2aa12d63040d7b60448dc449b7cc6"
       ],
       "layout": "IPY_MODEL_a372d2ca55da443ab6e8aae9f29934f1",
       "tabbable": null,
       "tooltip": null
      }
     },
     "348fb90e32534922ae866a89c1535cea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_548558cadb21425fa3fef9305a927579",
       "max": 7,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3543bcb390944447adc4386a7e5ba51f",
       "tabbable": null,
       "tooltip": null,
       "value": 7
      }
     },
     "3543bcb390944447adc4386a7e5ba51f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "3ca0c22c7fb941f783d6b92de7b73668": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "548558cadb21425fa3fef9305a927579": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "99c2aa12d63040d7b60448dc449b7cc6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1256232a33fa4888a7f96230439e2c76",
       "placeholder": "",
       "style": "IPY_MODEL_bfe67bb1596d48cabb27d337b9657ef5",
       "tabbable": null,
       "tooltip": null,
       "value": "7/7[03:49&lt;00:00,51.89s/it]"
      }
     },
     "9bd3cf423c1f4d308c5a9c8d7cb2de68": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a372d2ca55da443ab6e8aae9f29934f1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "bfe67bb1596d48cabb27d337b9657ef5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "f456bd28218b48ca8a0abb1a163306e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9bd3cf423c1f4d308c5a9c8d7cb2de68",
       "placeholder": "",
       "style": "IPY_MODEL_3ca0c22c7fb941f783d6b92de7b73668",
       "tabbable": null,
       "tooltip": null,
       "value": "Besttrial:2.Bestvalue:0.662272:100%"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
